name: run-analysis

on:
  workflow_dispatch:

jobs:
  run-figures:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install .[test]
          pip install seaborn matplotlib adjustText requests

      - name: Download analysis artefacts
        shell: bash
        run: |
          set -euo pipefail

          base='https://sharedspace.s3.msi.umn.edu'

          mapfile -t URLS <<'EOF'
          https://sharedspace.s3.msi.umn.edu/public_internet/variants_freeze4inv_sv_inv_hg38_processed_arbigent_filtered_manualDotplot_filtered_PAVgenAdded_withInvCategs_syncWithWH.fixedPH.simpleINV.mod.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/pairwise_glm_contrasts_fdr.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/pairwise_results_fdr.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/paml_results.checkpoint.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/perm_identical_pairs.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/perm_pairwise_identity.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/phy_metadata.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/region_identical_proportions.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/category_means_at_mean_covariates.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/category_summary.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_emm.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_emm_adjusted.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_emm_nocov.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_identical_proportions.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_pairwise.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_pairwise_adjusted.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/cds_pairwise_nocov.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/fixed_diff_summary.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/gene_direct_inverted.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/gene_inversion_direct_inverted.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/gene_inversion_fixed_differences.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/gene_inversion_permutation.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/glm_category_coefs.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/inv_info.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/inversion_fst_estimates.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/inversion_level_counts.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/inversion_level_medians.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/kruskal_result.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/fst_tests_summary.csv
          https://sharedspace.s3.msi.umn.edu/public_internet/inversion_statistical_results.csv
          https://sharedspace.s3.msi.umn.edu/public_internet/inv_info.csv
          https://sharedspace.s3.msi.umn.edu/public_internet/phewas_results.tsv
          https://sharedspace.s3.msi.umn.edu/public_internet/all_pairwise_results.csv
          EOF

          mkdir -p analysis_downloads
          : > analysis_downloads/.downloaded_files

          for url in "${URLS[@]}"; do
            rel="${url#${base}/}"
            out="analysis_downloads/${rel}"
            mkdir -p "$(dirname "$out")"
            echo "Fetching $url"
            if ! curl -fSL --retry 3 "$url" -o "$out"; then
              echo "Warning: failed to fetch $url; continuing"
              rm -f "$out"
              continue
            fi
            echo "$out" >> analysis_downloads/.downloaded_files
          done

          while IFS= read -r f; do
            echo "===== HEAD: $f ====="
            head -n 10 "$f" || true
            echo
          done < analysis_downloads/.downloaded_files

      - name: Download manual VCF workflow artefacts
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          python - <<'PY'
          import io
          import os
          import shutil
          import tempfile
          import zipfile
          from pathlib import Path

          import requests

          repo = os.environ.get("GITHUB_REPOSITORY")
          token = os.environ.get("GITHUB_TOKEN")
          if not repo:
              raise SystemExit("GITHUB_REPOSITORY environment variable is required")
          if not token:
              raise SystemExit("GITHUB_TOKEN environment variable is required to download artefacts")

          workflow_file = "manual_run_vcf.yml"
          session = requests.Session()
          session.headers.update(
              {
                  "Authorization": f"Bearer {token}",
                  "Accept": "application/vnd.github+json",
                  "X-GitHub-Api-Version": "2022-11-28",
              }
          )

          def api_get(url: str, **params):
              response = session.get(url, params=params)
              response.raise_for_status()
              return response

          runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/{workflow_file}/runs"
          runs = api_get(runs_url, status="success", exclude_pull_requests="true", per_page=1).json().get("workflow_runs", [])
          if not runs:
              raise SystemExit("No successful runs found for manual_run_vcf.yml")

          run = runs[0]
          run_id = run["id"]
          print(f"Using manual_run_vcf.yml artefacts from run {run_id} ({run['html_url']})")

          artifacts_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts"
          artifacts = {
              artifact["name"]: artifact
              for artifact in api_get(artifacts_url, per_page=100).json().get("artifacts", [])
          }

          required = {
              "run-vcf-output-csv": "output.csv",
              "run-vcf-falsta": None,
              "run-vcf-phy-outputs": None,
              "run-vcf-hudson-fst": "hudson_fst_results.tsv",
          }

          missing = [name for name in required if name not in artifacts]
          if missing:
              raise SystemExit(f"Missing required artefacts from manual_run_vcf.yml: {', '.join(missing)}")

          repo_root = Path(".").resolve()
          downloads_root = repo_root / "analysis_downloads"
          public_root = downloads_root / "public_internet"
          public_root.mkdir(parents=True, exist_ok=True)
          download_log = downloads_root / ".downloaded_files"
          download_log.parent.mkdir(parents=True, exist_ok=True)
          if not download_log.exists():
              download_log.touch()

          def download_artifact(artifact):
              url = artifact["archive_download_url"]
              print(f"Downloading artefact: {artifact['name']}")
              resp = session.get(url)
              resp.raise_for_status()
              return zipfile.ZipFile(io.BytesIO(resp.content))

          log_entries = []
          phy_written = 0

          # output.csv
          with download_artifact(artifacts["run-vcf-output-csv"]) as zf:
              for member in zf.namelist():
                  if member.endswith("output.csv"):
                      target = public_root / "output.csv"
                      target.parent.mkdir(parents=True, exist_ok=True)
                      with target.open("wb") as fh:
                          fh.write(zf.read(member))
                      log_entries.append(target)
                      print(f"Wrote {target.relative_to(repo_root)}")
                      break
              else:
                  raise SystemExit("output.csv not found in run-vcf-output-csv artefact")

          # FALSTA files
          with download_artifact(artifacts["run-vcf-falsta"]) as zf:
              extracted = False
              for member in zf.namelist():
                  if member.endswith(".falsta"):
                      target = public_root / Path(member).name
                      target.parent.mkdir(parents=True, exist_ok=True)
                      with target.open("wb") as fh:
                          fh.write(zf.read(member))
                      log_entries.append(target)
                      print(f"Wrote {target.relative_to(repo_root)}")
                      extracted = True
              if not extracted:
                  raise SystemExit("No .falsta files found in run-vcf-falsta artefact")

          # Hudson FST results
          with download_artifact(artifacts["run-vcf-hudson-fst"]) as zf:
              for member in zf.namelist():
                  if member.endswith("hudson_fst_results.tsv"):
                      target = public_root / "hudson_fst_results.tsv"
                      target.parent.mkdir(parents=True, exist_ok=True)
                      with target.open("wb") as fh:
                          fh.write(zf.read(member))
                      log_entries.append(target)
                      print(f"Wrote {target.relative_to(repo_root)}")
                      break
              else:
                  raise SystemExit("hudson_fst_results.tsv not found in run-vcf-hudson-fst artefact")

          # PHY files
          with download_artifact(artifacts["run-vcf-phy-outputs"]) as zf:
              members = zf.namelist()
              if not members:
                  raise SystemExit("run-vcf-phy-outputs artefact is empty")
              tmpdir = Path(tempfile.mkdtemp())
              try:
                  zf.extractall(tmpdir)
                  inner_archives = list(tmpdir.glob("*.zip"))
                  if not inner_archives:
                      raise SystemExit("No zip archives found inside run-vcf-phy-outputs artefact")
                  for archive in inner_archives:
                      with zipfile.ZipFile(archive, "r") as inner:
                          if archive.name == "phy_outputs.zip":
                              phy_zip_bytes = archive.read_bytes()
                              phy_zip_dest = downloads_root / "manual_run_vcf" / "phy_files.zip"
                              phy_zip_dest.parent.mkdir(parents=True, exist_ok=True)
                              phy_zip_dest.write_bytes(phy_zip_bytes)
                              shutil.copy2(phy_zip_dest, repo_root / "phy_files.zip")
                              print(
                                  f"Stored phy_files.zip at {phy_zip_dest.relative_to(repo_root)} and repository root"
                              )
                          for inner_member in inner.namelist():
                              if inner_member.endswith(".phy"):
                                  filename = Path(inner_member).name
                                  data = inner.read(inner_member)
                                  targets = [
                                      public_root / filename,
                                      public_root / "all_phy" / filename,
                                  ]
                                  for target in targets:
                                      target.parent.mkdir(parents=True, exist_ok=True)
                                      with target.open("wb") as fh:
                                          fh.write(data)
                                  root_target = repo_root / filename
                                  with root_target.open("wb") as fh:
                                      fh.write(data)
                                  phy_written += 1
                                  print(f"Extracted {filename} from phy_outputs.zip")
              finally:
                  shutil.rmtree(tmpdir)

          if phy_written:
              print(f"Extracted {phy_written} PHY files from manual_run_vcf artefact")

          with download_log.open("a", encoding="utf-8") as fh:
              for path in log_entries:
                  try:
                      fh.write(str(path.relative_to(repo_root)) + "\n")
                  except ValueError:
                      fh.write(str(path) + "\n")
          PY

      - name: Run replicate_figures.py
        id: run-figures
        shell: bash
        run: |
          status=0
          python scripts/replicate_figures.py --skip-downloads || status=$?
          echo "replicate_figures_exit_code=$status" >> "$GITHUB_OUTPUT"
          if [[ $status -ne 0 ]]; then
            echo "replicate_figures.py exited with status $status but continuing to allow artifact upload."
          fi

      - name: Post-run diagnostics
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          echo '=== Repository root contents ==='
          ls -al
          echo '=== Generated figures (top level) ==='
          find . -maxdepth 1 -type f \( -name '*.png' -o -name '*.pdf' -o -name '*.svg' \) -print
          echo '=== Generated figures (within subdirectories) ==='
          find stats -maxdepth 3 -type f \( -name '*.png' -o -name '*.pdf' -o -name '*.svg' \) -print || true
          echo '=== Download directory structure ==='
          find analysis_downloads -maxdepth 2 -type f -print
          echo '=== Disk usage summary ==='
          du -h --max-depth=1

      - name: Collect generated outputs
        if: always()
        run: |
          python - <<'PY'
          import shutil
          from pathlib import Path

          root = Path('.')
          figures_dest = Path('artifacts/figures')
          figures_dest.mkdir(parents=True, exist_ok=True)
          skip_dirs = {'analysis_downloads', 'artifacts'}
          for pattern in ('*.png', '*.pdf', '*.svg', '*.txt', '*.tsv', '*.csv'):
              for path in root.rglob(pattern):
                  if path.is_file():
                      if any(part in skip_dirs for part in path.parts):
                          continue
                      rel = path.relative_to(root)
                      target = figures_dest / rel
                      target.parent.mkdir(parents=True, exist_ok=True)
                      shutil.copy2(path, target)

          downloads_src = Path('analysis_downloads')
          downloads_dest = Path('artifacts/downloads')
          if downloads_src.exists():
              if downloads_dest.exists():
                  shutil.rmtree(downloads_dest)
              shutil.copytree(downloads_src, downloads_dest)
          PY

      - name: Upload figure artefacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: figure-outputs
          path: artifacts/figures
          if-no-files-found: warn

      - name: Upload downloaded data artefacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: analysis-downloads
          path: artifacts/downloads
          if-no-files-found: warn

